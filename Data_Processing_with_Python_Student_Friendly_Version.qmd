---
title: "Data_Processing_with_Python_Student_Friendly_Version"
format: html
editor: visual
---

Libraries that will be imported in this document. These may require installment in Terminal using "pip install":

```{python}
import pandas as pd # importing the pandas package to manipulate dataframes
import re # importing the re message to use regular expressions on text
from siuba import count
import plotly.express as px # importing plotly.express to make interactive plots
import nltk
nltk.download("punkt")
from nltk.corpus import stopwords
import iocextract
import requests # to make requests to websites
import json # to read or load json files
import plotly.graph_objects as go # required to make the map
import tidytext
from tidytext import unnest_tokens, bind_tf_idf # to run unnest_token, plyr may have to be installed on terminal
from wordcloud import WordCloud
import matplotlib.pyplot as plt # required to show the wordcloud
from nltk.corpus import words
import pyvis
from pyvis.network import Network
```

## Introduction

In this Quarto document, I will reproduce some of the work for this research project on the Conti Ransomware Group's chat logs, which we first coded in the R programming language, now in the Python programming language.

This version is meant to be usable by students. It should be reproducible, simple to browse, and well-explained. Because the original data set was too large and would cause some of the codes to run more slowly, only a smaller sample of the data set will be used here.

The first code chunk will set the folder or directory that this quarto document will work in. Without later specifying a different folder, only the files in this folder can be read, and any files written will be saved in this folder:

```{r setup}
    knitr::opts_knit$set(root.dir = normalizePath("C:\\Users\\jliu2\\Documents\\R_Scripts"))
```

The code chunk above will be the only one written in R. Please replace the directory inside quotation marks with the directory of the folder you would like to work in, replacing single backslashes with double backslashes. We can used the *getwd()* function to check what folder we are in. If the code above does not work, then we can go to the *Tools* tab, *Global Options*, and change the default working directory.

Some of the following codes will also be based on what is in a blog post written by *Microsoft 365 Defender Research Team*, who did a project on the same topic. A link to their post: https://www.microsoft.com/security/blog/2022/06/01/using-python-to-unearth-a-goldmine-of-threat-intelligence-from-leaked-chat-logs/

## Data

Using a function from the *pandas* package, we will import the dataset into RStudio.

```{python}
import pandas as pd # importing the pandas package to manipulate dataframes

# We will start by importing the unedited dataset xlsx file in the current directory, using the read_excel() function to read an xlsx file:
df = pd.read_excel("json_files_combined.xlsx")

# Checking what type the data has been imported into:
print("The dataset has been imported as a", type(df))
```

```{python}
# Checking the length of the dataframe:
print("This dataframe is", len(df), "rows long.")
```

```{python}
# 60,800 rows is too long for learning purposes, so we will instead take a random sample 1/10 of the length: 
sample = df.sample(frac=0.1)
print("The length of the sample dataset is", len(sample))
```

After having rows randomly taken from it, the order in the dataframe had been ruin. To fix that, we can use the sort_values() function:

```{python}
sample = sample.sort_values(by = "ts") # sorting the dataframe's rows by the values in its timestamp column from earliest to latest

sample.head(5) # checking the first 5 rows of the dataframe
```

Sometimes, we would want to find or perform operations on certain segments of text from a larger string or a dataset. For that, the functions from the *re* package, and regular expressions, are useful.

Regular expressions represent text, or multiple instances of texts. Different functions and different programming languages may take regular expressions slightly differently, but they are usually written similarly. For the following regular expressions, a letter or number by itself would represent just itself in the regular expression, but if something is put between brackets \[\], that would represent the presence of 1 or more if the \[\] are followed by a + sign, and 0 or more if followed by a \* sign, of any one thing in the brackets.

```{python}
import re # importing the re message to use regular expressions on text

sample["ts"] = sample["ts"].str.replace(r"T[0-9]*:[0-9]*:[0-9]*.[0-9]*", "", regex=True) # using the replace function & a regular expression to remove hour, minute, seconds, from the timestamp column

sample["from"] = sample["from"].str.replace(r"[@.]+[A-Za-z0-9.]*", "", regex=True) # using the replace function & a regular expression to remove extensions from the usernames of the hackers who sent messages

sample["to"] = sample["to"].str.replace(r"[@.]+[A-Za-z0-9.]*", "", regex=True) # using the replace function & a regular expression to remove extensions from the usernames of the hackers who received messages

# creating a dictionary with slang words used by the hackers, paired with their translations
slang = {"Hell":"AD", "YES":"DA", "wheelbarrow":"host", "cars":"hosts", "cue balls":"bitcoin", "cue ball":"bitcoin", "credits":"credentials", "vmik":"WMIC", "grid":"network", "facial expressions":"mimikatz", "fireworks":"firewall", "whining":"SQL", "school":"SQL", "balls":"shares", "zithers":"Citrix", "food":"FUD", "silkcode":"shellcode", "kosh":"cash", "toad":"jabber", "booze":"Emotet", "the trick or trick":"Trickbot", "BC":"BazarBackdoor", "backpack":"Ryuk"} 
# using the "slang" dictionary to replace slang words with understandable english words: 
sample["body"] = sample["body"].replace(slang, regex=True) 

sample.head(5)
```

## Activity Timeline

The Microsoft 365 Defender Research Team made a Activeness Timeline for the Conti Ransomware group, using data on the number of message their members sent each day, and the *bokeh* visualization library, which can create visualizations that can be inserted into HTML pages.

I will instead use the *plotly* visualization library, because it can produce similar visualizations, but requires less coding and will not produce errors when repeatedly running similar codes.

But before making the visualization, we need to put the data in a suitable form:

```{python}
# importing count() function from the siuba package
from siuba import count

sample_ts_count = count(sample, "ts") # counting the number of messages for each timestamp

sample_ts_count["ts"] = pd.to_datetime(sample_ts_count["ts"]) # turning the timestamp column from string to datetime datatype

sample_ts_count.head(5)
```

Now to plot the time series, as a line plot:

```{python}
import plotly.express as px # importing plotly.express to make interactive plots

# Creating a figure for a line chart:
fig1 = px.line(sample_ts_count, x = "ts", y = "n", title = "Conti Discussions in 2021", labels = {"n":"Number of Messages Sent", "ts":"Date"}) # A dictionary was used to replace the default x & y labels on the plot

fig1.show() # opens up the plot in your web browser
```

Observation: In the interactive line plot, there is a toolbar at the top right of the page with multiple icons. By selecting:

-   "Zoom", we can zoom in by clicking & dragging over the line plot

-   "Pan", we can move the plot around

-   "Reset axes", we can return to seeing the entire plot

## User Activity

Although they used a different code, the Microsoft 365 Defender Research Team also made a simple horizontal bar graph for individual user activity.

```{python}
sample_from_count = count(sample, "from") # making a dataframe with the number of messages sent by each user under the "from" column

sample_from_count = sample_from_count.sort_values(by = 'n') # sorting the rows by high to low "n" counts

# Creating a figure for a horizontal bar plot:
fig2 = px.bar(sample_from_count, x = "n", y = "from", orientation='h', title = "Conti Hacker Activeness", labels = {"from":"Username", "n":"Number of Messages sent"}) # Orientation was set to "h" for horizontal, and a dictionary was used to replace the default x & y labels on the plot.

fig2.show() # opens up the plot in your web browser
```

Observation: For this plot, without zooming in far enough, we will not be able to see all of the usernames on the y axis.

## Searching for IOCs

IOCs are *Indicators of Compromise*, items related to cyber threats. Impressively, there is a package, *icoextract*, with functions meant to find from strings of text, different kinds of IOCs, like ip addresses, urls, bitcoin addresses, etc. The functions from icoextract however, produce *itertools chain objects*, which can only accessed through use of iteration (Ex. For loops).

Next, we will search for ip addresses and the messages that they appeared in. They will be put into a nested list, which has lists within a list. This is one of the formats that pandas's DataFrame() function was designed to be capable of transforming into a dataframe.

```{python}
import iocextract

ipv4s = [] # initializing a list to hold internet protocols 

for i in sample["body"]: # looping through the entries in the "body" column of the dataframe
  for ips in iocextract.extract_ipv4s(data = i, refang = False): # for every ip address (4th version) found by the extract_ipv4s() function in this row...
    ipv4s.append([ips, i]) # append the ip address found & the message it came from, into the initialized list, as a list itself

ip_vs_text = pd.DataFrame(ipv4s, columns = ["ip","body"]) # turning the nested list into a dataframe with custom column names
ip_vs_text = ip_vs_text.drop_duplicates(subset='ip', keep="first") # dropping rows with duplicate ip addresses, keeping only the first of these duplicates

ip_vs_text.head(5)
```

Explanation: It is possible that the same ip address may come from different rows of text, and these different rows of text may contain valuable information. But in the above code chunk, every duplicate ip address aside from the first was removed, along with the texts corresponding to them, because for what we will do next, duplicate ip addresses are not necessary.

Having the ip addresses alone doesn't tell us much. So next, we will try to map the ip addresses by finding their corresponding latitudes and longitudes. There are a few databases online that can be used to match ip addresses to real-world locations. The *requests* packages has functions that allow us to get data from those websites. The one being used in the following code chunk is: *https://geolocation-db.com/*

```{python}
import requests # to make requests to websites
import json # to read or load json files

ip_info = [] # initializing a list to carry information about each ip address

for ip in ip_vs_text["ip"]: 
  # making the url string to which we will send the request by combining the website & an ip address:
  request_url = 'https://geolocation-db.com/jsonp/' + ip 
  # sending the request to get data back from the url & putting the response (data) in a variable:
  response = requests.get(request_url)
  # decoding the response:
  result = response.content.decode()
  # the decoded response is in the form of a dictionary with parentheses around it. Stripping the outer parentheses away:
  result = result.split("(")[1].strip(")")
  # reading the response's text and turning it into an actual dictionary:
  result = json.loads(result)
  # taking only the results we may be interested in and appending them into a nested list:
  ip_info.append([result['country_code'], result['country_name'], result['city'], result['postal'], result['latitude'], result['longitude'], result['IPv4'], result['state']])

# Turning the nested list into a dataframe with custom column names:
ip_info = pd.DataFrame(ip_info, columns=['country_code', 'country_name', 'city', 'postal', 'latitude', 'longitude', 'ip', 'state']) 

ip_info.head(5)
```

There are rows in the dataframe where no information, not even the ip address is found. These rows will be removed:

```{python}
ip_info_found = ip_info[ip_info['ip'] != "IP Not found"] # getting only the rows where ip address does not equal "IP Not found"
```

With longitude and latitude, we can make these ip addresses points on a graph. If possible, we want there to be some information provided when we hover over these points. That is why we will create a "text" column to contain customize text that we want to show:

```{python}
ip_info_found["text"] = ip_info_found["country_name"] + ", " + ip_info_found["city"] 
```

Making a map using *plotly* package:

```{python}
import plotly.graph_objects as go # required to make the map

fig3 = go.Figure(data = go.Scattergeo(
        lon = ip_info_found['longitude'], # telling the function which column has the longitude values
        lat = ip_info_found['latitude'], # telling the function which column has the latitude values
        text = ip_info_found['text'], # setting the text to show when mouse hovers over the points
        mode = 'markers' # deciding the drawing mode
        ))

fig3.update_layout( # updating the figure's layout
        title = 'Possible targets of Conti Group', # adding a title
    )

fig3.update_geos(
    projection_type="orthographic", # makes the world map turn from flat to a globe
    showcountries=True, countrycolor="Black", # gives country lines to the world map
    #showcoastlines=True, coastlinecolor="Brown",
    showocean=True, oceancolor="LightBlue",
    #showlakes=True, lakecolor="LightBlue",
    #showrivers=True, rivercolor="LightBlue"
) # if rendered in RStudio, this line of code will produce a visualization too

#fig3.show() # opens the figure in browser
```

Out of interest for what webpages conti hackers visit, we also searched for urls in the dataset:

```{python}
urls = [] # initializing a list for urls

for i in sample["body"]: # looping through the "body" column of the dataset
  for url in iocextract.extract_urls(data = i, refang = False): # # this lines means: for every url found by the extract_url() function in this row...
    urls.append([url, i]) # nesting each url and the text where the url was found, as a list, to the list

url_vs_text = pd.DataFrame(urls, columns=["url", "body"]) # Turning the nested list into a dataframe with custom column names
url_vs_text = url_vs_text.drop_duplicates(keep="first") # dropping duplicate rows, only keeping the first

url_vs_text.head(5)
```

Note: Although duplicate rows were removed, the same url may still appear in rows with different text under the "body" column.

## Frequency Analysis

Most of the information in the Conti chat logs is in their messages. But there are many messages, and the lengths and subjects of the messages can change erratically. Checking each message one by one will be time-consuming and difficult. The *tidytext* package contains functions that make it easier to analyze text in dataframes. For example: - *unnest_tokens()*, which splits cells of text in dataframes into one-token-per-row while preserving the data in other columns - *bind_tf_idf()*, which can calculate the frequency of terms and find possibly important words The *tidytext* library exists in both R and Python. It can be found at: *https://pypi.org/project/tidytext/*

But before we perform frequency analysis on the words, we have to consider duplicate text. Duplicate text among the rows may cause wrong results in frequency analysis. For example, when one casual message is sent to everyone in a group chat, the counts of the words in that message would skyrocket. We will remove the duplicates, but considering that text might just be coincidentally repeated over time, we will only remove rows with duplicate text from the same day:

```{python}
sample_unique = sample.drop_duplicates(['ts','body'], keep = 'first')
```

```{python}
import nltk
nltk.download("punkt") # downloading a sentence tokenizer needed to run tidytext function in python, which only has to be done once

import tidytext
from tidytext import unnest_tokens, bind_tf_idf # to run unnest_token, plyr may have to be installed on terminal

sample_unnested = unnest_tokens(sample_unique, "word", "body") # unnesting the text in the "body" column into single tokens under a "word" column

sample_unnested.head(5)
```

There are words that have the same base but different ending. For example: crypt & crypts. These words have the same meaning, but will be counted separately. This is unless we "stem" them, cutting off the extra part, and only leaving them with their base:

```{python}
from nltk.stem.snowball import SnowballStemmer # importing a stemming algorithm from nltk

stemmer = SnowballStemmer("english") # selecting english words from the multiple languages that come with SnowballStemmer

sample_unnested["word"] = sample_unnested["word"].astype(str).apply(lambda x: stemmer.stem(x)) # applying the stem() function to every cell in the word column turned into string datatype

sample_unnested
```

We still can't determine much from seeing the dataframe as it is now. Next, we will count how many times each token or term appeared in the dataset:

```{python}
sample_word_count = count(sample_unnested, "word")

sample_word_count
```

Observation: There are many words that have relatively less meaning; For example, stopwords, frequently used words like: "a", "to", "are". And for our analysis, words that do not appear often probably won't have much weight. Such words can be filtered out.

```{python}
from nltk.corpus import stopwords # importing stopwords (contains various languages of stopwords)

stopwords = stopwords.words('english') # getting a list of english stopwords

sample_word_count_nostop = sample_word_count[~sample_word_count['word'].isin(stopwords)] # removing any row whose word is in the list of stopwords

sample_word_count_nostop2 = sample_word_count_nostop[sample_word_count_nostop["n"] > 1] # filtering for words that appear more than once

sample_word_count_nostop2
```

There are also a lot of words that seem meaningless. The may represent something, but we are unable to tell. Instead, they will only get in the way of our text analysis. It is too difficult to figure out a regular expression that can represent and remove all these incomprehensible terms.

So instead, to get only comprehensible words from the dataframe, the terms in the table will be matched to an english word list from *nltk*:

```{python}
from nltk.corpus import words

word_list = words.words() # getting a list of 236759 english words from nltk

# adding custom words to the word_list. These are the translations of the previously mentioned slang words, and some words that may not be in the nltk word list.
word_list = word_list + ["AD", "DA", "host", "hosts", "bitcoin", "crypto", "cryptocurrency", "credentials", "WMIC", "network", "mimikatz", "firewall", "SQL", "shares", "Citrix", "FUD", "shellcode", "cash", "jabber", "Emotet", "Trickbot", "BazarBackdoor", "Ryuk", "decrypted", "decrypt", "message", "bro", "error", "encrypted", "encrypt", "trends", "admin"] 

sample_word_count_nostop2 = sample_word_count_nostop2[sample_word_count_nostop2['word'].isin(word_list)] # removing any row whose words are not in the word_list

sample_word_count_nostop2
```

If we can compare the frequency of the words, then we may be able to tell what subjects are more important to the hackers, or what they focus more on. To indicate the frequency of each word, we can use a Wordcloud. To use the *WordCloud()* function from the *wordcloud* package, we need to input all our words as a string. In the following code chunk, we will assemble that string and make a Wordcloud:

```{python}
all_words_string = " ".join(sample_word_count_nostop2["word"]) # joining all the words in the list of all words into a single string with a single space in between each word

from wordcloud import WordCloud
import matplotlib.pyplot as plt # required to show the wordcloud

plt.subplots(figsize = (8,8)) # setting/reserving a space to put the figure that will contain the WordCloud

wordcloud = WordCloud ( # making the WordCloud
                    background_color = 'white', # setting the background color
                    width = 1000, # setting the width of the figure
                    height = 1000, # setting the heigth of the figure
                    max_words = 100, # setting the maximum number of words that can enter the word list
                    collocations = False # this argument prevent words from being repeated in the wordcloud
                        ).generate(all_words_string) # telling the function which string of words to build the wordcloud from
                        
plt.imshow(wordcloud) # to read/input the wordcloud data into an image
plt.axis('off') # to prevent x & y axes from appearing
#plt.savefig('Plotly_Word_Cloud.png') # to save the figure as a png file in the current directory
plt.show() # shows the figure as an output
```

But it isn't always certain that the most frequently mentioned words are the most important words. To find the most important words, we will try calculating *tf_idf*.

"*tf_idf*" is the *product* of *term frequency* and *inverse document frequency*. It decreases for words that are more commonly found in the documents and increases for words that are more rarely used. It can be used to rank the importance of the terms.

The *bind_tf_idf()* function calculates tf_idf. But to use it, we must first have a column in our dataframe to treat as documents (in the following code check, we will use dates), and counts of each word:

```{python}
# Counting the number of times each word appears each day:
sample_word_per_file = count(sample_unnested, "ts", "word")

# Calculating tf_idf using the bind_tf_idf() function:
sample_tf_idf = bind_tf_idf(sample_word_per_file, "word", "ts", "n")

# Sorting dataframe by descending tf_idf:
sample_tf_idf_sorted = sample_tf_idf.sort_values(by='tf_idf', ascending = False)

sample_tf_idf_sorted.head()
```

Next, we'll remove the stopwords:

```{python}
sample_tf_idf_nostop = sample_tf_idf_sorted[~sample_tf_idf_sorted['word'].isin(stopwords)] # removing any row whose words are in the list of stopwords

sample_tf_idf_nostop["word"].head(5) 
```

There are still too many unreadable terms. Since we can't interpret anything from them right now, we will remove them:

```{python}
sample_tf_idf_nostop2 = sample_tf_idf_nostop[sample_tf_idf_nostop['word'].isin(word_list)] # removing any row whose words are not in the word_list

sample_tf_idf_nostop2.head()
```

Using the *plotly* package to create a horizontal bar plot of the words with the highest tf_idf and possibly highest importance:

```{python}
import plotly.express as px

# Creating the bar plot's figure
fig4 = px.bar(sample_tf_idf_nostop2.head(30), x = "tf_idf", y = "word", orientation='h', title = "Possibly Important Words in Conti Messages", labels = {"word":"Word", "tf_idf":"Importance by tf_idf"}) # setting orientation to "h" for a horizontal bar plot and using a python dictionary to replace x & y labels

fig4.show() # opens up the plot in your web browser
```

## Network

A *Network* is a vizualization/graph that shows the relationships between nodes. - Nodes are points on the graph that can each represent a term. - Edges are lines that represent a connection between the two nodes. They can show which term precedes or come after another term.

A network would be a good way to visualize the relationship between the hackers. To visualize networks, we will be using functions from the *Pyvis* library. But to gain some control over the size of the network, we will make it so that it only shows the connection possess by specified groups/tiers of users. To pave the way for that, we will first sort our dataset by those who sent the most messages, to those who sent the least:

```{python}
sample_from = count(sample, "from") # counting the number of rows for each user in the "from" column

sample_from_sorted = sample_from.sort_values(by = 'n', ascending = False) # sorting the rows by high to low "n" counts
```

Because we want to create multiple networks, but do not want to rewrite the same code multiple times, the code to create the network(s) will be made into a function. This function will require the sample & sample_from_sorted dataframes to be already loaded, the username extensions to have already been removed, and the pyvis library to have already been imported. For its inputs, it will accepts 2 numbers to specify which users in the sample_from_sorted dataframe we want to see in the network, a color for the nodes in the network, and a color for the lines in the network.

```{python}
import pyvis
from pyvis.network import Network

def conti_network(start_num, end_num, node_color, edge_color): 
  
  # finding the specified users ranging from start number to end number
  specified_users = list(sample_from_sorted[start_num : end_num]["from"])
  
  # Counting the number of messages sent from each user to the others:
  sample_weight = sample.groupby(["from", "to"], as_index = False).count()
  # Renaming a column:
  sample_weight.columns = sample_weight.columns.str.replace("ts", "weight")
  # Keeping only those rows where the specified users sent messages: 
  sample_weight = sample_weight[sample_weight["from"].isin(specified_users)]
  
  # Configuring network/graph
  conti_net = Network(height = '800px', width = '100%', bgcolor = '#222222', # '#222222' for black
  font_color = 'white', notebook = True) 

  conti_net.barnes_hut() # Setting physics layout of the network
  
  # Creating data for the network's lines using the "from", "to", & "weight" (former "n") columns:
  sources = sample_weight['from']
  targets = sample_weight['to']
  weights = sample_weight['weight']

  edge_data = zip(sources, targets, weights) # creates an iterable zip object from the 3 columns' data

  for e in edge_data:
    src = e[0] # getting data from the first "column" (formerly "from") of the zip object
    dst = e[1] # getting data from the second "column" (formerly "to")  of the zip object
    w = e[2] # getting data from the third "column" (formerly "weight", or "n") of the zip object
    conti_net.add_node(src, src, title = src, color = node_color) # adding a node for the source (from)
    conti_net.add_node(dst, dst, title = dst, color = node_color) # adding a node for the destination (to)
    conti_net.add_edge(src, dst, value = w * 10, color = edge_color) # adding a line from source to destination

  neighbor_map = conti_net.get_adj_list() # getting a dictionary of id(s) of connected nodes

  # Adding user data to node table:
  for node in conti_net.nodes:
    node['value'] = len(neighbor_map[node["id"]]) # adding to the node, a count of the number of connected nodes 
    node['title'] = node['title'] + ' sent to or received messages from ' + str(node['value']) + ' of the present users' # changing the text in the title column
    
  conti_net.show('conti_network.html') # creating the network html file in the current directory
```

Running the function:

```{python}
conti_network(0, 5, 'white', 'red') # Creating a network for the 5 users that sent the highest number of messages, with red nodes and white lines
```

Observation: The produced Network can show many more nodes than the number of users specified in the input, because the network will also make nodes, not only for the users who sent messages, but also for the users who messages were sent to. Text will appear when our mouse pointer hovers over any of the nodes, but if the network is too big, it will be necessary to zoom in before our pointer can accurately make contact with the node and have it provide the text.

Problems:

-   When taking only a part of the dataset, the data shown by the network would only show numbers based on that part. The numbers may not be the same if a different part of the dataset is used.

-   This code, which was largely based off of the Microsoft 365 Defender Research Team's, purely counts connections for each node. This makes it harder for us to determine whether the connections are coming from, or going to, a node. That is why instead, the text that appears when we hover over a node says "sent to or received from".

-   The produced html file can take a while to load. The higher the number of users inputted, the longer it will take. After repeatedly running the function, it may or may not show a loading bar.

-   Running this function a second time would replace the conti_network.html file in the current folder. Renaming the previous conti_network.html file will prevent that.
